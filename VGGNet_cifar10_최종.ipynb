{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67JZccmeOd0j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqLHV2JY6Iz_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0onDVOWzDVU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import warnings\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUwuF-yrLY0L"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageDraw\n",
        "from IPython.display import SVG\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import xml.etree.ElementTree as ET\n",
        "from xml.etree.ElementTree import Element, ElementTree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4dhX3nCLabs"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist,cifar10\n",
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D,BatchNormalization ,Conv2D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam,SGD\n",
        "from tensorflow.keras.utils import to_categorical,plot_model\n",
        "from tensorflow.keras.losses import categorical_crossentropy,binary_crossentropy\n",
        "from tensorflow.keras.callbacks import Callback,LearningRateScheduler\n",
        "from tensorflow.python.client import device_lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPx9fbZWMOiK",
        "outputId": "e32f598b-a3b1-4ed2-9857-4b7909786850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting download\n",
            "  Downloading download-0.3.5-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from download) (4.66.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from download) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from download) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->download) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->download) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->download) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->download) (2024.2.2)\n",
            "Installing collected packages: download\n",
            "Successfully installed download-0.3.5\n"
          ]
        }
      ],
      "source": [
        "!pip install download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmHTYL9VMqYT",
        "outputId": "4513f4e7-35f4-4d15-8d43-f0d25c167091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dataset\n",
            "  Downloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
            "Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n",
            "  Downloading SQLAlchemy-1.4.52-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=0.6.2 (from dataset)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting banal>=1.0.1 (from dataset)\n",
            "  Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
            "Collecting Mako (from alembic>=0.6.2->dataset)\n",
            "  Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (4.10.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=0.6.2->dataset) (2.1.5)\n",
            "Installing collected packages: banal, sqlalchemy, Mako, alembic, dataset\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.28\n",
            "    Uninstalling SQLAlchemy-2.0.28:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.28\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.52 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.2 alembic-1.13.1 banal-1.0.6 dataset-1.6.2 sqlalchemy-1.4.52\n"
          ]
        }
      ],
      "source": [
        "!pip install dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5a2tU0-Le17"
      },
      "outputs": [],
      "source": [
        "# CIFAR-10 클래스\n",
        "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "# CIFAR-10 데이터 불러오기\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Train / Validation 데이터 분리하기\n",
        "num_train = int(x_train.shape[0] * 0.8)\n",
        "num_val = x_train.shape[0] - num_train\n",
        "\n",
        "# Validation 데이터 생성\n",
        "x_val = x_train[num_train:]\n",
        "y_val = y_train[num_train:]\n",
        "\n",
        "# Train 데이터 수정\n",
        "x_train = x_train[:num_train]\n",
        "y_train = y_train[:num_train]\n"
      ],
      "metadata": {
        "id": "33-mQFGbN0at",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5beb15ff-d009-4db1-c6c1-f3c00f889f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_val = keras.utils.to_categorical(y_val, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtvtsxvPQk46",
        "outputId": "87b85600-9827-4b6c-aac7-0ac444d037fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40000, 1)\n",
            "(10000, 1)\n",
            "(10000, 1)\n",
            "(40000, 10)\n",
            "(10000, 10)\n",
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjl5jwWFM7Uy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "# from data_utils import *\n",
        "import tensorflow_datasets as tfds\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize']=(10.0,8.0)\n",
        "plt.rcParams['image.interpolation']='nearest'\n",
        "plt.rcParams['image.cmap']='gray'\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "import os\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTtmXMNXNC8i"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras import Input\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPool2D\n",
        "from keras.layers import Activation\n",
        "from keras import regularizers\n",
        "from keras.layers import AveragePooling2D\n",
        "from keras.layers import GlobalAveragePooling2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRnp0DmJNW7S"
      },
      "outputs": [],
      "source": [
        "def conv2d_bn_relu(model, filters, block_index, layer_index,weight_decay=.0,padding='same'):\n",
        "  conv_name = 'conv'+str(block_index)+'-'+str(layer_index)\n",
        "  model = Conv2D(filters=filters, kernel_size=(3, 3), padding=padding, kernel_regularizer=regularizers.l2(weight_decay),strides=(1,1),name=conv_name,)(model)\n",
        "  bn_name = 'bn'+str(block_index)+'-'+str(layer_index)\n",
        "  model = BatchNormalization(name=bn_name)(model)\n",
        "  relu_name = 'relu'+str(block_index)+'-'+str(layer_index)\n",
        "  model = Activation('relu',name=relu_name)(model)\n",
        "  return model\n",
        "def dense2d_bn_dropout(model,units,weight_decay,name):\n",
        "  model = Dense(units, kernel_regularizer=regularizers.l2(weight_decay),name=name,)(model)\n",
        "  model = BatchNormalization(name=name+'-bn')(model)\n",
        "  model = Activation('relu',name=name+'-relu')(model)\n",
        "  model = Dropout(0.5,name=name+'-dropout')(model)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y56T5B3MNcKi"
      },
      "outputs": [],
      "source": [
        "def VGGNet(classes,input_shape,weight_decay,conv_block_num=5,fc_layers=2,fc_units=4096):\n",
        "  input = Input(shape=input_shape)\n",
        "  #block1\n",
        "  x = conv2d_bn_relu(model=input,filters=64,block_index=1,layer_index=1,weight_decay=weight_decay)\n",
        "  x = tf.keras.layers.add([x, Dropout(0.3)(x)])\n",
        "  x = conv2d_bn_relu(model=x,filters=64,block_index=1,layer_index=2,weight_decay=weight_decay)\n",
        "  x = MaxPool2D(name='pool1')(x)\n",
        "  #block2\n",
        "  if conv_block_num >= 2:\n",
        "    x = conv2d_bn_relu(x,filters=128,block_index=2,layer_index=1,weight_decay=weight_decay)\n",
        "    x = tf.keras.layers.add([x, Dropout(0.4)(x)])\n",
        "    x = conv2d_bn_relu(x,filters=128,block_index=2,layer_index=2,weight_decay=weight_decay)\n",
        "    x = MaxPool2D(name='pool2')(x)\n",
        "  #block3\n",
        "  if conv_block_num >= 3:\n",
        "    x = conv2d_bn_relu(x,filters=256,block_index=3,layer_index=1,weight_decay=weight_decay)\n",
        "    x = tf.keras.layers.add([x, Dropout(0.4)(x)])\n",
        "    x = conv2d_bn_relu(x,filters=256,block_index=3,layer_index=2,weight_decay=weight_decay)\n",
        "    x = tf.keras.layers.add([x, Dropout(0.4)(x)])\n",
        "    x = conv2d_bn_relu(x,filters=256,block_index=3,layer_index=3,weight_decay=weight_decay)\n",
        "    x = MaxPool2D(name='pool3')(x)\n",
        "  #block4\n",
        "  if conv_block_num >= 4:\n",
        "    x = conv2d_bn_relu(x,filters=512,block_index=4,layer_index=1,weight_decay=weight_decay)\n",
        "    x = tf.keras.layers.add([x, Dropout(0.4)(x)])\n",
        "    x = conv2d_bn_relu(x,filters=512,block_index=4,layer_index=2,weight_decay=weight_decay)\n",
        "    x = tf.keras.layers.add([x, Dropout(0.4)(x)])\n",
        "    x = conv2d_bn_relu(x,filters=512,block_index=4,layer_index=3,weight_decay=weight_decay)\n",
        "    x = MaxPool2D(name='pool4')(x)\n",
        "  #block5\n",
        "  if conv_block_num >= 5:\n",
        "    x = conv2d_bn_relu(x,filters=512,block_index=5,layer_index=1,weight_decay=weight_decay)\n",
        "    x = tf.keras.layers.add([x, Dropout(0.4)(x)])\n",
        "    x = conv2d_bn_relu(x,filters=512,block_index=5,layer_index=2,weight_decay=weight_decay)\n",
        "    x = tf.keras.layers.add([x, Dropout(0.4)(x)])\n",
        "    x = conv2d_bn_relu(x,filters=512,block_index=5,layer_index=3,weight_decay=weight_decay)\n",
        "    x = MaxPool2D(name='pool5')(x)\n",
        "\n",
        "  x = Flatten(name='flatten')(x)\n",
        "  if fc_layers>=1:\n",
        "    x = dense2d_bn_dropout(x,fc_units,weight_decay,name='fc6')\n",
        "    if fc_layers>=2:\n",
        "      x = dense2d_bn_dropout(x,fc_units,weight_decay,name='fc7')\n",
        "  out = Dense(classes,activation='softmax',name='predictions')(x)\n",
        "  model = Model(input,out)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f6vmmGHNeBD"
      },
      "outputs": [],
      "source": [
        "def VGG16(classes):\n",
        "  return VGGNet(classes,weight_decay=1e-6,conv_block_num=5,fc_layers=2,fc_units=4096) # weight_decay수정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6xpI2fqOpcp",
        "outputId": "80c630d6-726a-411d-b05d-612baa7094f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " conv1-1 (Conv2D)            (None, 32, 32, 64)           1792      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " bn1-1 (BatchNormalization)  (None, 32, 32, 64)           256       ['conv1-1[0][0]']             \n",
            "                                                                                                  \n",
            " relu1-1 (Activation)        (None, 32, 32, 64)           0         ['bn1-1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 32, 32, 64)           0         ['relu1-1[0][0]']             \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 32, 32, 64)           0         ['relu1-1[0][0]',             \n",
            "                                                                     'dropout[0][0]']             \n",
            "                                                                                                  \n",
            " conv1-2 (Conv2D)            (None, 32, 32, 64)           36928     ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " bn1-2 (BatchNormalization)  (None, 32, 32, 64)           256       ['conv1-2[0][0]']             \n",
            "                                                                                                  \n",
            " relu1-2 (Activation)        (None, 32, 32, 64)           0         ['bn1-2[0][0]']               \n",
            "                                                                                                  \n",
            " pool1 (MaxPooling2D)        (None, 16, 16, 64)           0         ['relu1-2[0][0]']             \n",
            "                                                                                                  \n",
            " conv2-1 (Conv2D)            (None, 16, 16, 128)          73856     ['pool1[0][0]']               \n",
            "                                                                                                  \n",
            " bn2-1 (BatchNormalization)  (None, 16, 16, 128)          512       ['conv2-1[0][0]']             \n",
            "                                                                                                  \n",
            " relu2-1 (Activation)        (None, 16, 16, 128)          0         ['bn2-1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 16, 16, 128)          0         ['relu2-1[0][0]']             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 16, 16, 128)          0         ['relu2-1[0][0]',             \n",
            "                                                                     'dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " conv2-2 (Conv2D)            (None, 16, 16, 128)          147584    ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " bn2-2 (BatchNormalization)  (None, 16, 16, 128)          512       ['conv2-2[0][0]']             \n",
            "                                                                                                  \n",
            " relu2-2 (Activation)        (None, 16, 16, 128)          0         ['bn2-2[0][0]']               \n",
            "                                                                                                  \n",
            " pool2 (MaxPooling2D)        (None, 8, 8, 128)            0         ['relu2-2[0][0]']             \n",
            "                                                                                                  \n",
            " conv3-1 (Conv2D)            (None, 8, 8, 256)            295168    ['pool2[0][0]']               \n",
            "                                                                                                  \n",
            " bn3-1 (BatchNormalization)  (None, 8, 8, 256)            1024      ['conv3-1[0][0]']             \n",
            "                                                                                                  \n",
            " relu3-1 (Activation)        (None, 8, 8, 256)            0         ['bn3-1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 8, 8, 256)            0         ['relu3-1[0][0]']             \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 8, 8, 256)            0         ['relu3-1[0][0]',             \n",
            "                                                                     'dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv3-2 (Conv2D)            (None, 8, 8, 256)            590080    ['add_2[0][0]']               \n",
            "                                                                                                  \n",
            " bn3-2 (BatchNormalization)  (None, 8, 8, 256)            1024      ['conv3-2[0][0]']             \n",
            "                                                                                                  \n",
            " relu3-2 (Activation)        (None, 8, 8, 256)            0         ['bn3-2[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 8, 8, 256)            0         ['relu3-2[0][0]']             \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 8, 8, 256)            0         ['relu3-2[0][0]',             \n",
            "                                                                     'dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " conv3-3 (Conv2D)            (None, 8, 8, 256)            590080    ['add_3[0][0]']               \n",
            "                                                                                                  \n",
            " bn3-3 (BatchNormalization)  (None, 8, 8, 256)            1024      ['conv3-3[0][0]']             \n",
            "                                                                                                  \n",
            " relu3-3 (Activation)        (None, 8, 8, 256)            0         ['bn3-3[0][0]']               \n",
            "                                                                                                  \n",
            " pool3 (MaxPooling2D)        (None, 4, 4, 256)            0         ['relu3-3[0][0]']             \n",
            "                                                                                                  \n",
            " conv4-1 (Conv2D)            (None, 4, 4, 512)            1180160   ['pool3[0][0]']               \n",
            "                                                                                                  \n",
            " bn4-1 (BatchNormalization)  (None, 4, 4, 512)            2048      ['conv4-1[0][0]']             \n",
            "                                                                                                  \n",
            " relu4-1 (Activation)        (None, 4, 4, 512)            0         ['bn4-1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 4, 4, 512)            0         ['relu4-1[0][0]']             \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 4, 4, 512)            0         ['relu4-1[0][0]',             \n",
            "                                                                     'dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " conv4-2 (Conv2D)            (None, 4, 4, 512)            2359808   ['add_4[0][0]']               \n",
            "                                                                                                  \n",
            " bn4-2 (BatchNormalization)  (None, 4, 4, 512)            2048      ['conv4-2[0][0]']             \n",
            "                                                                                                  \n",
            " relu4-2 (Activation)        (None, 4, 4, 512)            0         ['bn4-2[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 4, 4, 512)            0         ['relu4-2[0][0]']             \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 4, 4, 512)            0         ['relu4-2[0][0]',             \n",
            "                                                                     'dropout_5[0][0]']           \n",
            "                                                                                                  \n",
            " conv4-3 (Conv2D)            (None, 4, 4, 512)            2359808   ['add_5[0][0]']               \n",
            "                                                                                                  \n",
            " bn4-3 (BatchNormalization)  (None, 4, 4, 512)            2048      ['conv4-3[0][0]']             \n",
            "                                                                                                  \n",
            " relu4-3 (Activation)        (None, 4, 4, 512)            0         ['bn4-3[0][0]']               \n",
            "                                                                                                  \n",
            " pool4 (MaxPooling2D)        (None, 2, 2, 512)            0         ['relu4-3[0][0]']             \n",
            "                                                                                                  \n",
            " conv5-1 (Conv2D)            (None, 2, 2, 512)            2359808   ['pool4[0][0]']               \n",
            "                                                                                                  \n",
            " bn5-1 (BatchNormalization)  (None, 2, 2, 512)            2048      ['conv5-1[0][0]']             \n",
            "                                                                                                  \n",
            " relu5-1 (Activation)        (None, 2, 2, 512)            0         ['bn5-1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 2, 2, 512)            0         ['relu5-1[0][0]']             \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 2, 2, 512)            0         ['relu5-1[0][0]',             \n",
            "                                                                     'dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            " conv5-2 (Conv2D)            (None, 2, 2, 512)            2359808   ['add_6[0][0]']               \n",
            "                                                                                                  \n",
            " bn5-2 (BatchNormalization)  (None, 2, 2, 512)            2048      ['conv5-2[0][0]']             \n",
            "                                                                                                  \n",
            " relu5-2 (Activation)        (None, 2, 2, 512)            0         ['bn5-2[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 2, 2, 512)            0         ['relu5-2[0][0]']             \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 2, 2, 512)            0         ['relu5-2[0][0]',             \n",
            "                                                                     'dropout_7[0][0]']           \n",
            "                                                                                                  \n",
            " conv5-3 (Conv2D)            (None, 2, 2, 512)            2359808   ['add_7[0][0]']               \n",
            "                                                                                                  \n",
            " bn5-3 (BatchNormalization)  (None, 2, 2, 512)            2048      ['conv5-3[0][0]']             \n",
            "                                                                                                  \n",
            " relu5-3 (Activation)        (None, 2, 2, 512)            0         ['bn5-3[0][0]']               \n",
            "                                                                                                  \n",
            " pool5 (MaxPooling2D)        (None, 1, 1, 512)            0         ['relu5-3[0][0]']             \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 512)                  0         ['pool5[0][0]']               \n",
            "                                                                                                  \n",
            " fc6 (Dense)                 (None, 512)                  262656    ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " fc6-bn (BatchNormalization  (None, 512)                  2048      ['fc6[0][0]']                 \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " fc6-relu (Activation)       (None, 512)                  0         ['fc6-bn[0][0]']              \n",
            "                                                                                                  \n",
            " fc6-dropout (Dropout)       (None, 512)                  0         ['fc6-relu[0][0]']            \n",
            "                                                                                                  \n",
            " fc7 (Dense)                 (None, 512)                  262656    ['fc6-dropout[0][0]']         \n",
            "                                                                                                  \n",
            " fc7-bn (BatchNormalization  (None, 512)                  2048      ['fc7[0][0]']                 \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " fc7-relu (Activation)       (None, 512)                  0         ['fc7-bn[0][0]']              \n",
            "                                                                                                  \n",
            " fc7-dropout (Dropout)       (None, 512)                  0         ['fc7-relu[0][0]']            \n",
            "                                                                                                  \n",
            " predictions (Dense)         (None, 10)                   5130      ['fc7-dropout[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15266122 (58.24 MB)\n",
            "Trainable params: 15255626 (58.20 MB)\n",
            "Non-trainable params: 10496 (41.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "weight_decay=5e-4\n",
        "lr = 0.1 #learning_rate수정\n",
        "num_classes =10\n",
        "\n",
        "vgg = VGGNet(classes = num_classes,input_shape=x_train.shape[1:],weight_decay=weight_decay,conv_block_num=5,fc_layers=2,fc_units=512)\n",
        "\n",
        "#sgd\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9, nesterov=True)#nesterov(False->True)수정\n",
        "vgg.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "vgg.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7M4SGa1NfvS",
        "outputId": "33dd42b9-09d7-4b98-a7da-cecbfc025ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 27 µs, sys: 0 ns, total: 27 µs\n",
            "Wall time: 30.3 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "data_augmentation = True\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  return lr * (0.5 ** (epoch//20)) #lr_drop수정\n",
        "reduce_lr = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "batch_size =128\n",
        "epochs = 250"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if data_augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,\n",
        "        samplewise_center=False,\n",
        "        featurewise_std_normalization=False,\n",
        "        samplewise_std_normalization=False,\n",
        "        zca_whitening=False,\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=False\n",
        "    )\n",
        "    print('train with data_augumentation')\n",
        "    history = vgg.fit_generator(generator=datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                                steps_per_epoch=len(x_train) // batch_size,\n",
        "                                epochs=epochs,\n",
        "                                callbacks=[reduce_lr],\n",
        "                                validation_data=(x_val, y_val))\n",
        "else:\n",
        "    print('train without data_augumentation')\n",
        "    history = vgg.fit(x_train, y_train, batch_size=batch_size,\n",
        "                      epochs=epochs,\n",
        "                      callbacks=[reduce_lr],\n",
        "                      validation_data=(x_val, y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMhU2_HoPJ0d",
        "outputId": "e8702090-0b4a-4c56-c398-41422e1e5bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train with data_augumentation\n",
            "Epoch 1/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-e85a00949fe3>:15: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = vgg.fit_generator(generator=datagen.flow(x_train, y_train, batch_size=batch_size),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "312/312 [==============================] - 53s 125ms/step - loss: 6.8371 - accuracy: 0.1752 - val_loss: 5.6048 - val_accuracy: 0.1512 - lr: 0.1000\n",
            "Epoch 2/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 4.3162 - accuracy: 0.2924 - val_loss: 4.1549 - val_accuracy: 0.1390 - lr: 0.1000\n",
            "Epoch 3/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 3.0690 - accuracy: 0.3609 - val_loss: 3.3164 - val_accuracy: 0.1698 - lr: 0.1000\n",
            "Epoch 4/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 2.3536 - accuracy: 0.4268 - val_loss: 2.4513 - val_accuracy: 0.3030 - lr: 0.1000\n",
            "Epoch 5/250\n",
            "312/312 [==============================] - 32s 104ms/step - loss: 1.9277 - accuracy: 0.5010 - val_loss: 2.0141 - val_accuracy: 0.4257 - lr: 0.1000\n",
            "Epoch 6/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 1.6571 - accuracy: 0.5707 - val_loss: 1.8188 - val_accuracy: 0.5186 - lr: 0.1000\n",
            "Epoch 7/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 1.5119 - accuracy: 0.6134 - val_loss: 2.2735 - val_accuracy: 0.4480 - lr: 0.1000\n",
            "Epoch 8/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 1.4246 - accuracy: 0.6455 - val_loss: 1.4708 - val_accuracy: 0.6240 - lr: 0.1000\n",
            "Epoch 9/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 1.3864 - accuracy: 0.6660 - val_loss: 1.6959 - val_accuracy: 0.5664 - lr: 0.1000\n",
            "Epoch 10/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 1.3625 - accuracy: 0.6806 - val_loss: 1.3398 - val_accuracy: 0.6739 - lr: 0.1000\n",
            "Epoch 11/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 1.3365 - accuracy: 0.6968 - val_loss: 1.6310 - val_accuracy: 0.6276 - lr: 0.1000\n",
            "Epoch 12/250\n",
            "312/312 [==============================] - 33s 106ms/step - loss: 1.3176 - accuracy: 0.7110 - val_loss: 1.5127 - val_accuracy: 0.6321 - lr: 0.1000\n",
            "Epoch 13/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 1.3105 - accuracy: 0.7221 - val_loss: 1.4036 - val_accuracy: 0.6738 - lr: 0.1000\n",
            "Epoch 14/250\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 1.2984 - accuracy: 0.7303 - val_loss: 2.5276 - val_accuracy: 0.4490 - lr: 0.1000\n",
            "Epoch 15/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 1.2911 - accuracy: 0.7368 - val_loss: 1.4328 - val_accuracy: 0.6705 - lr: 0.1000\n",
            "Epoch 16/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 1.2878 - accuracy: 0.7421 - val_loss: 1.3308 - val_accuracy: 0.7120 - lr: 0.1000\n",
            "Epoch 17/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 1.2803 - accuracy: 0.7454 - val_loss: 1.3067 - val_accuracy: 0.7236 - lr: 0.1000\n",
            "Epoch 18/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 1.2905 - accuracy: 0.7487 - val_loss: 1.5635 - val_accuracy: 0.6456 - lr: 0.1000\n",
            "Epoch 19/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 1.2839 - accuracy: 0.7528 - val_loss: 1.5975 - val_accuracy: 0.6444 - lr: 0.1000\n",
            "Epoch 20/250\n",
            "312/312 [==============================] - 35s 110ms/step - loss: 1.2773 - accuracy: 0.7555 - val_loss: 2.1299 - val_accuracy: 0.5255 - lr: 0.1000\n",
            "Epoch 21/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 1.1146 - accuracy: 0.7984 - val_loss: 1.1886 - val_accuracy: 0.7680 - lr: 0.0500\n",
            "Epoch 22/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 1.0438 - accuracy: 0.8081 - val_loss: 1.2255 - val_accuracy: 0.7526 - lr: 0.0500\n",
            "Epoch 23/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 1.0136 - accuracy: 0.8117 - val_loss: 1.2754 - val_accuracy: 0.7199 - lr: 0.0500\n",
            "Epoch 24/250\n",
            "312/312 [==============================] - 32s 104ms/step - loss: 1.0046 - accuracy: 0.8125 - val_loss: 1.2221 - val_accuracy: 0.7462 - lr: 0.0500\n",
            "Epoch 25/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.9973 - accuracy: 0.8128 - val_loss: 1.0165 - val_accuracy: 0.7960 - lr: 0.0500\n",
            "Epoch 26/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 1.0047 - accuracy: 0.8142 - val_loss: 1.0105 - val_accuracy: 0.8125 - lr: 0.0500\n",
            "Epoch 27/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.9990 - accuracy: 0.8173 - val_loss: 1.2563 - val_accuracy: 0.7297 - lr: 0.0500\n",
            "Epoch 28/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.9982 - accuracy: 0.8189 - val_loss: 1.5448 - val_accuracy: 0.6703 - lr: 0.0500\n",
            "Epoch 29/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 1.0091 - accuracy: 0.8198 - val_loss: 1.1579 - val_accuracy: 0.7680 - lr: 0.0500\n",
            "Epoch 30/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 1.0100 - accuracy: 0.8197 - val_loss: 0.9824 - val_accuracy: 0.8211 - lr: 0.0500\n",
            "Epoch 31/250\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 1.0141 - accuracy: 0.8242 - val_loss: 1.3958 - val_accuracy: 0.7165 - lr: 0.0500\n",
            "Epoch 32/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 1.0071 - accuracy: 0.8233 - val_loss: 1.2759 - val_accuracy: 0.7230 - lr: 0.0500\n",
            "Epoch 33/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 1.0021 - accuracy: 0.8279 - val_loss: 1.2850 - val_accuracy: 0.7441 - lr: 0.0500\n",
            "Epoch 34/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.9999 - accuracy: 0.8284 - val_loss: 1.4942 - val_accuracy: 0.6774 - lr: 0.0500\n",
            "Epoch 35/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 1.0105 - accuracy: 0.8290 - val_loss: 1.4187 - val_accuracy: 0.7129 - lr: 0.0500\n",
            "Epoch 36/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 1.0061 - accuracy: 0.8297 - val_loss: 1.0682 - val_accuracy: 0.8044 - lr: 0.0500\n",
            "Epoch 37/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 1.0157 - accuracy: 0.8288 - val_loss: 1.2368 - val_accuracy: 0.7622 - lr: 0.0500\n",
            "Epoch 38/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 1.0142 - accuracy: 0.8340 - val_loss: 1.4611 - val_accuracy: 0.7012 - lr: 0.0500\n",
            "Epoch 39/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 1.0130 - accuracy: 0.8321 - val_loss: 1.2265 - val_accuracy: 0.7665 - lr: 0.0500\n",
            "Epoch 40/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 1.0168 - accuracy: 0.8323 - val_loss: 1.3566 - val_accuracy: 0.7073 - lr: 0.0500\n",
            "Epoch 41/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.8904 - accuracy: 0.8694 - val_loss: 0.8763 - val_accuracy: 0.8531 - lr: 0.0250\n",
            "Epoch 42/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.8259 - accuracy: 0.8771 - val_loss: 0.8585 - val_accuracy: 0.8620 - lr: 0.0250\n",
            "Epoch 43/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.7988 - accuracy: 0.8783 - val_loss: 1.0457 - val_accuracy: 0.8069 - lr: 0.0250\n",
            "Epoch 44/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.7930 - accuracy: 0.8784 - val_loss: 0.8920 - val_accuracy: 0.8412 - lr: 0.0250\n",
            "Epoch 45/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.7858 - accuracy: 0.8742 - val_loss: 0.9886 - val_accuracy: 0.8150 - lr: 0.0250\n",
            "Epoch 46/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.7806 - accuracy: 0.8759 - val_loss: 0.8706 - val_accuracy: 0.8510 - lr: 0.0250\n",
            "Epoch 47/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.7704 - accuracy: 0.8804 - val_loss: 0.8722 - val_accuracy: 0.8430 - lr: 0.0250\n",
            "Epoch 48/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.7797 - accuracy: 0.8761 - val_loss: 0.8390 - val_accuracy: 0.8563 - lr: 0.0250\n",
            "Epoch 49/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.7749 - accuracy: 0.8791 - val_loss: 1.0902 - val_accuracy: 0.7961 - lr: 0.0250\n",
            "Epoch 50/250\n",
            "312/312 [==============================] - 33s 106ms/step - loss: 0.7767 - accuracy: 0.8789 - val_loss: 0.9810 - val_accuracy: 0.8182 - lr: 0.0250\n",
            "Epoch 51/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.7648 - accuracy: 0.8822 - val_loss: 1.0478 - val_accuracy: 0.7931 - lr: 0.0250\n",
            "Epoch 52/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.7688 - accuracy: 0.8804 - val_loss: 1.0530 - val_accuracy: 0.7995 - lr: 0.0250\n",
            "Epoch 53/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.7693 - accuracy: 0.8803 - val_loss: 1.0648 - val_accuracy: 0.7972 - lr: 0.0250\n",
            "Epoch 54/250\n",
            "312/312 [==============================] - 32s 104ms/step - loss: 0.7745 - accuracy: 0.8818 - val_loss: 1.2638 - val_accuracy: 0.7595 - lr: 0.0250\n",
            "Epoch 55/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.7808 - accuracy: 0.8791 - val_loss: 0.9167 - val_accuracy: 0.8336 - lr: 0.0250\n",
            "Epoch 56/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.7709 - accuracy: 0.8837 - val_loss: 1.2049 - val_accuracy: 0.7473 - lr: 0.0250\n",
            "Epoch 57/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.7824 - accuracy: 0.8788 - val_loss: 0.9986 - val_accuracy: 0.8054 - lr: 0.0250\n",
            "Epoch 58/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.7836 - accuracy: 0.8815 - val_loss: 1.0489 - val_accuracy: 0.8033 - lr: 0.0250\n",
            "Epoch 59/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.7764 - accuracy: 0.8843 - val_loss: 1.3037 - val_accuracy: 0.7356 - lr: 0.0250\n",
            "Epoch 60/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.7750 - accuracy: 0.8839 - val_loss: 1.0238 - val_accuracy: 0.8116 - lr: 0.0250\n",
            "Epoch 61/250\n",
            "312/312 [==============================] - 36s 114ms/step - loss: 0.6881 - accuracy: 0.9119 - val_loss: 0.7626 - val_accuracy: 0.8815 - lr: 0.0125\n",
            "Epoch 62/250\n",
            "312/312 [==============================] - 36s 114ms/step - loss: 0.6424 - accuracy: 0.9199 - val_loss: 0.7742 - val_accuracy: 0.8799 - lr: 0.0125\n",
            "Epoch 63/250\n",
            "312/312 [==============================] - 36s 114ms/step - loss: 0.6244 - accuracy: 0.9220 - val_loss: 0.6930 - val_accuracy: 0.8965 - lr: 0.0125\n",
            "Epoch 64/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.6055 - accuracy: 0.9249 - val_loss: 0.7544 - val_accuracy: 0.8757 - lr: 0.0125\n",
            "Epoch 65/250\n",
            "312/312 [==============================] - 36s 114ms/step - loss: 0.6002 - accuracy: 0.9230 - val_loss: 0.7399 - val_accuracy: 0.8800 - lr: 0.0125\n",
            "Epoch 66/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.5939 - accuracy: 0.9240 - val_loss: 0.7749 - val_accuracy: 0.8672 - lr: 0.0125\n",
            "Epoch 67/250\n",
            "312/312 [==============================] - 36s 114ms/step - loss: 0.5881 - accuracy: 0.9222 - val_loss: 0.7139 - val_accuracy: 0.8873 - lr: 0.0125\n",
            "Epoch 68/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.5854 - accuracy: 0.9233 - val_loss: 0.8367 - val_accuracy: 0.8523 - lr: 0.0125\n",
            "Epoch 69/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.5850 - accuracy: 0.9217 - val_loss: 0.7242 - val_accuracy: 0.8812 - lr: 0.0125\n",
            "Epoch 70/250\n",
            "312/312 [==============================] - 33s 107ms/step - loss: 0.5827 - accuracy: 0.9201 - val_loss: 0.8735 - val_accuracy: 0.8406 - lr: 0.0125\n",
            "Epoch 71/250\n",
            "312/312 [==============================] - 36s 114ms/step - loss: 0.5854 - accuracy: 0.9207 - val_loss: 0.7000 - val_accuracy: 0.8872 - lr: 0.0125\n",
            "Epoch 72/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.5777 - accuracy: 0.9213 - val_loss: 0.8780 - val_accuracy: 0.8355 - lr: 0.0125\n",
            "Epoch 73/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.5790 - accuracy: 0.9206 - val_loss: 0.8105 - val_accuracy: 0.8572 - lr: 0.0125\n",
            "Epoch 74/250\n",
            "312/312 [==============================] - 32s 104ms/step - loss: 0.5727 - accuracy: 0.9230 - val_loss: 0.7676 - val_accuracy: 0.8673 - lr: 0.0125\n",
            "Epoch 75/250\n",
            "312/312 [==============================] - 36s 114ms/step - loss: 0.5743 - accuracy: 0.9235 - val_loss: 0.9076 - val_accuracy: 0.8306 - lr: 0.0125\n",
            "Epoch 76/250\n",
            "312/312 [==============================] - 33s 106ms/step - loss: 0.5838 - accuracy: 0.9208 - val_loss: 0.7596 - val_accuracy: 0.8708 - lr: 0.0125\n",
            "Epoch 77/250\n",
            "312/312 [==============================] - 32s 104ms/step - loss: 0.5874 - accuracy: 0.9208 - val_loss: 0.7052 - val_accuracy: 0.8830 - lr: 0.0125\n",
            "Epoch 78/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.5788 - accuracy: 0.9225 - val_loss: 0.8165 - val_accuracy: 0.8496 - lr: 0.0125\n",
            "Epoch 79/250\n",
            "312/312 [==============================] - 33s 106ms/step - loss: 0.5791 - accuracy: 0.9216 - val_loss: 0.9022 - val_accuracy: 0.8374 - lr: 0.0125\n",
            "Epoch 80/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.5761 - accuracy: 0.9234 - val_loss: 0.7810 - val_accuracy: 0.8629 - lr: 0.0125\n",
            "Epoch 81/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.5101 - accuracy: 0.9455 - val_loss: 0.6339 - val_accuracy: 0.9050 - lr: 0.0063\n",
            "Epoch 82/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.4823 - accuracy: 0.9512 - val_loss: 0.6543 - val_accuracy: 0.9003 - lr: 0.0063\n",
            "Epoch 83/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.4736 - accuracy: 0.9524 - val_loss: 0.6492 - val_accuracy: 0.9025 - lr: 0.0063\n",
            "Epoch 84/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.4617 - accuracy: 0.9528 - val_loss: 0.6559 - val_accuracy: 0.9000 - lr: 0.0063\n",
            "Epoch 85/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.4507 - accuracy: 0.9557 - val_loss: 0.5972 - val_accuracy: 0.9132 - lr: 0.0063\n",
            "Epoch 86/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.4480 - accuracy: 0.9544 - val_loss: 0.6357 - val_accuracy: 0.9028 - lr: 0.0063\n",
            "Epoch 87/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.4368 - accuracy: 0.9571 - val_loss: 0.7669 - val_accuracy: 0.8754 - lr: 0.0063\n",
            "Epoch 88/250\n",
            "312/312 [==============================] - 34s 108ms/step - loss: 0.4406 - accuracy: 0.9546 - val_loss: 0.6247 - val_accuracy: 0.9069 - lr: 0.0063\n",
            "Epoch 89/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.4343 - accuracy: 0.9551 - val_loss: 0.6437 - val_accuracy: 0.8983 - lr: 0.0063\n",
            "Epoch 90/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.4267 - accuracy: 0.9571 - val_loss: 0.6161 - val_accuracy: 0.9092 - lr: 0.0063\n",
            "Epoch 91/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.4238 - accuracy: 0.9554 - val_loss: 0.6287 - val_accuracy: 0.9055 - lr: 0.0063\n",
            "Epoch 92/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.4262 - accuracy: 0.9539 - val_loss: 0.6177 - val_accuracy: 0.9055 - lr: 0.0063\n",
            "Epoch 93/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.4250 - accuracy: 0.9542 - val_loss: 0.6757 - val_accuracy: 0.8853 - lr: 0.0063\n",
            "Epoch 94/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.4267 - accuracy: 0.9536 - val_loss: 0.6000 - val_accuracy: 0.9066 - lr: 0.0063\n",
            "Epoch 95/250\n",
            "312/312 [==============================] - 32s 104ms/step - loss: 0.4189 - accuracy: 0.9556 - val_loss: 0.7116 - val_accuracy: 0.8799 - lr: 0.0063\n",
            "Epoch 96/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.4210 - accuracy: 0.9530 - val_loss: 0.6107 - val_accuracy: 0.9028 - lr: 0.0063\n",
            "Epoch 97/250\n",
            "312/312 [==============================] - 35s 110ms/step - loss: 0.4081 - accuracy: 0.9581 - val_loss: 0.6094 - val_accuracy: 0.9031 - lr: 0.0063\n",
            "Epoch 98/250\n",
            "312/312 [==============================] - 35s 114ms/step - loss: 0.4183 - accuracy: 0.9543 - val_loss: 0.5924 - val_accuracy: 0.9064 - lr: 0.0063\n",
            "Epoch 99/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.4125 - accuracy: 0.9550 - val_loss: 0.7025 - val_accuracy: 0.8853 - lr: 0.0063\n",
            "Epoch 100/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.4091 - accuracy: 0.9562 - val_loss: 0.7351 - val_accuracy: 0.8715 - lr: 0.0063\n",
            "Epoch 101/250\n",
            "312/312 [==============================] - 32s 104ms/step - loss: 0.3803 - accuracy: 0.9656 - val_loss: 0.5746 - val_accuracy: 0.9114 - lr: 0.0031\n",
            "Epoch 102/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.3547 - accuracy: 0.9719 - val_loss: 0.5637 - val_accuracy: 0.9188 - lr: 0.0031\n",
            "Epoch 103/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.3490 - accuracy: 0.9729 - val_loss: 0.5733 - val_accuracy: 0.9154 - lr: 0.0031\n",
            "Epoch 104/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.3435 - accuracy: 0.9728 - val_loss: 0.5561 - val_accuracy: 0.9201 - lr: 0.0031\n",
            "Epoch 105/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.3388 - accuracy: 0.9746 - val_loss: 0.6075 - val_accuracy: 0.9086 - lr: 0.0031\n",
            "Epoch 106/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.3335 - accuracy: 0.9741 - val_loss: 0.5810 - val_accuracy: 0.9116 - lr: 0.0031\n",
            "Epoch 107/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.3276 - accuracy: 0.9760 - val_loss: 0.6090 - val_accuracy: 0.9089 - lr: 0.0031\n",
            "Epoch 108/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.3196 - accuracy: 0.9780 - val_loss: 0.5638 - val_accuracy: 0.9202 - lr: 0.0031\n",
            "Epoch 109/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.3249 - accuracy: 0.9750 - val_loss: 0.6001 - val_accuracy: 0.9108 - lr: 0.0031\n",
            "Epoch 110/250\n",
            "312/312 [==============================] - 35s 110ms/step - loss: 0.3199 - accuracy: 0.9757 - val_loss: 0.5873 - val_accuracy: 0.9116 - lr: 0.0031\n",
            "Epoch 111/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.3147 - accuracy: 0.9768 - val_loss: 0.5719 - val_accuracy: 0.9135 - lr: 0.0031\n",
            "Epoch 112/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.3112 - accuracy: 0.9781 - val_loss: 0.5875 - val_accuracy: 0.9141 - lr: 0.0031\n",
            "Epoch 113/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.3152 - accuracy: 0.9752 - val_loss: 0.5852 - val_accuracy: 0.9141 - lr: 0.0031\n",
            "Epoch 114/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.3076 - accuracy: 0.9769 - val_loss: 0.5342 - val_accuracy: 0.9187 - lr: 0.0031\n",
            "Epoch 115/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.3096 - accuracy: 0.9765 - val_loss: 0.5824 - val_accuracy: 0.9138 - lr: 0.0031\n",
            "Epoch 116/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.3049 - accuracy: 0.9770 - val_loss: 0.5805 - val_accuracy: 0.9138 - lr: 0.0031\n",
            "Epoch 117/250\n",
            "312/312 [==============================] - 35s 110ms/step - loss: 0.3078 - accuracy: 0.9755 - val_loss: 0.5546 - val_accuracy: 0.9149 - lr: 0.0031\n",
            "Epoch 118/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.2975 - accuracy: 0.9779 - val_loss: 0.7557 - val_accuracy: 0.8827 - lr: 0.0031\n",
            "Epoch 119/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.3044 - accuracy: 0.9757 - val_loss: 0.5578 - val_accuracy: 0.9140 - lr: 0.0031\n",
            "Epoch 120/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.3035 - accuracy: 0.9741 - val_loss: 0.5854 - val_accuracy: 0.9089 - lr: 0.0031\n",
            "Epoch 121/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.2857 - accuracy: 0.9800 - val_loss: 0.5311 - val_accuracy: 0.9198 - lr: 0.0016\n",
            "Epoch 122/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.2693 - accuracy: 0.9852 - val_loss: 0.5199 - val_accuracy: 0.9226 - lr: 0.0016\n",
            "Epoch 123/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.2685 - accuracy: 0.9850 - val_loss: 0.5497 - val_accuracy: 0.9191 - lr: 0.0016\n",
            "Epoch 124/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.2639 - accuracy: 0.9860 - val_loss: 0.5292 - val_accuracy: 0.9240 - lr: 0.0016\n",
            "Epoch 125/250\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 0.2585 - accuracy: 0.9867 - val_loss: 0.5261 - val_accuracy: 0.9238 - lr: 0.0016\n",
            "Epoch 126/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.2584 - accuracy: 0.9873 - val_loss: 0.5446 - val_accuracy: 0.9191 - lr: 0.0016\n",
            "Epoch 127/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.2524 - accuracy: 0.9882 - val_loss: 0.5478 - val_accuracy: 0.9196 - lr: 0.0016\n",
            "Epoch 128/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.2525 - accuracy: 0.9872 - val_loss: 0.5249 - val_accuracy: 0.9229 - lr: 0.0016\n",
            "Epoch 129/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.2519 - accuracy: 0.9879 - val_loss: 0.5387 - val_accuracy: 0.9216 - lr: 0.0016\n",
            "Epoch 130/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.2493 - accuracy: 0.9876 - val_loss: 0.5300 - val_accuracy: 0.9221 - lr: 0.0016\n",
            "Epoch 131/250\n",
            "312/312 [==============================] - 33s 106ms/step - loss: 0.2496 - accuracy: 0.9870 - val_loss: 0.5309 - val_accuracy: 0.9248 - lr: 0.0016\n",
            "Epoch 132/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.2467 - accuracy: 0.9875 - val_loss: 0.5439 - val_accuracy: 0.9212 - lr: 0.0016\n",
            "Epoch 133/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.2454 - accuracy: 0.9878 - val_loss: 0.5317 - val_accuracy: 0.9213 - lr: 0.0016\n",
            "Epoch 134/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.2411 - accuracy: 0.9881 - val_loss: 0.5354 - val_accuracy: 0.9211 - lr: 0.0016\n",
            "Epoch 135/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.2413 - accuracy: 0.9879 - val_loss: 0.5465 - val_accuracy: 0.9181 - lr: 0.0016\n",
            "Epoch 136/250\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 0.2364 - accuracy: 0.9891 - val_loss: 0.5645 - val_accuracy: 0.9207 - lr: 0.0016\n",
            "Epoch 137/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.2383 - accuracy: 0.9881 - val_loss: 0.5327 - val_accuracy: 0.9224 - lr: 0.0016\n",
            "Epoch 138/250\n",
            "312/312 [==============================] - 35s 110ms/step - loss: 0.2390 - accuracy: 0.9880 - val_loss: 0.5816 - val_accuracy: 0.9149 - lr: 0.0016\n",
            "Epoch 139/250\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 0.2343 - accuracy: 0.9882 - val_loss: 0.5444 - val_accuracy: 0.9210 - lr: 0.0016\n",
            "Epoch 140/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.2374 - accuracy: 0.9878 - val_loss: 0.5707 - val_accuracy: 0.9146 - lr: 0.0016\n",
            "Epoch 141/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.2285 - accuracy: 0.9902 - val_loss: 0.5118 - val_accuracy: 0.9259 - lr: 7.8125e-04\n",
            "Epoch 142/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.2245 - accuracy: 0.9909 - val_loss: 0.5138 - val_accuracy: 0.9250 - lr: 7.8125e-04\n",
            "Epoch 143/250\n",
            "312/312 [==============================] - 32s 104ms/step - loss: 0.2198 - accuracy: 0.9920 - val_loss: 0.5149 - val_accuracy: 0.9245 - lr: 7.8125e-04\n",
            "Epoch 144/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.2186 - accuracy: 0.9921 - val_loss: 0.5187 - val_accuracy: 0.9258 - lr: 7.8125e-04\n",
            "Epoch 145/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.2154 - accuracy: 0.9926 - val_loss: 0.5077 - val_accuracy: 0.9299 - lr: 7.8125e-04\n",
            "Epoch 146/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.2141 - accuracy: 0.9935 - val_loss: 0.5154 - val_accuracy: 0.9271 - lr: 7.8125e-04\n",
            "Epoch 147/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.2138 - accuracy: 0.9932 - val_loss: 0.5184 - val_accuracy: 0.9272 - lr: 7.8125e-04\n",
            "Epoch 148/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.2145 - accuracy: 0.9923 - val_loss: 0.5255 - val_accuracy: 0.9251 - lr: 7.8125e-04\n",
            "Epoch 149/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.2132 - accuracy: 0.9924 - val_loss: 0.5234 - val_accuracy: 0.9265 - lr: 7.8125e-04\n",
            "Epoch 150/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.2125 - accuracy: 0.9920 - val_loss: 0.5309 - val_accuracy: 0.9238 - lr: 7.8125e-04\n",
            "Epoch 151/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.2115 - accuracy: 0.9925 - val_loss: 0.5055 - val_accuracy: 0.9275 - lr: 7.8125e-04\n",
            "Epoch 152/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.2083 - accuracy: 0.9935 - val_loss: 0.5134 - val_accuracy: 0.9279 - lr: 7.8125e-04\n",
            "Epoch 153/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.2102 - accuracy: 0.9929 - val_loss: 0.5220 - val_accuracy: 0.9242 - lr: 7.8125e-04\n",
            "Epoch 154/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.2060 - accuracy: 0.9934 - val_loss: 0.5219 - val_accuracy: 0.9251 - lr: 7.8125e-04\n",
            "Epoch 155/250\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 0.2069 - accuracy: 0.9933 - val_loss: 0.5247 - val_accuracy: 0.9241 - lr: 7.8125e-04\n",
            "Epoch 156/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.2028 - accuracy: 0.9943 - val_loss: 0.5254 - val_accuracy: 0.9282 - lr: 7.8125e-04\n",
            "Epoch 157/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.2064 - accuracy: 0.9933 - val_loss: 0.5328 - val_accuracy: 0.9259 - lr: 7.8125e-04\n",
            "Epoch 158/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.2035 - accuracy: 0.9941 - val_loss: 0.5179 - val_accuracy: 0.9262 - lr: 7.8125e-04\n",
            "Epoch 159/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.2037 - accuracy: 0.9928 - val_loss: 0.5278 - val_accuracy: 0.9270 - lr: 7.8125e-04\n",
            "Epoch 160/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.2020 - accuracy: 0.9935 - val_loss: 0.5081 - val_accuracy: 0.9292 - lr: 7.8125e-04\n",
            "Epoch 161/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.1970 - accuracy: 0.9949 - val_loss: 0.5158 - val_accuracy: 0.9275 - lr: 3.9063e-04\n",
            "Epoch 162/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.1978 - accuracy: 0.9947 - val_loss: 0.5162 - val_accuracy: 0.9290 - lr: 3.9063e-04\n",
            "Epoch 163/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.1967 - accuracy: 0.9946 - val_loss: 0.5180 - val_accuracy: 0.9266 - lr: 3.9063e-04\n",
            "Epoch 164/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1965 - accuracy: 0.9944 - val_loss: 0.5118 - val_accuracy: 0.9285 - lr: 3.9063e-04\n",
            "Epoch 165/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1950 - accuracy: 0.9951 - val_loss: 0.5133 - val_accuracy: 0.9277 - lr: 3.9063e-04\n",
            "Epoch 166/250\n",
            "312/312 [==============================] - 35s 110ms/step - loss: 0.1944 - accuracy: 0.9948 - val_loss: 0.5102 - val_accuracy: 0.9290 - lr: 3.9063e-04\n",
            "Epoch 167/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.1917 - accuracy: 0.9958 - val_loss: 0.5143 - val_accuracy: 0.9292 - lr: 3.9063e-04\n",
            "Epoch 168/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1919 - accuracy: 0.9956 - val_loss: 0.5052 - val_accuracy: 0.9293 - lr: 3.9063e-04\n",
            "Epoch 169/250\n",
            "312/312 [==============================] - 33s 106ms/step - loss: 0.1926 - accuracy: 0.9954 - val_loss: 0.5141 - val_accuracy: 0.9300 - lr: 3.9063e-04\n",
            "Epoch 170/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.1918 - accuracy: 0.9953 - val_loss: 0.5317 - val_accuracy: 0.9275 - lr: 3.9063e-04\n",
            "Epoch 171/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1910 - accuracy: 0.9956 - val_loss: 0.5202 - val_accuracy: 0.9266 - lr: 3.9063e-04\n",
            "Epoch 172/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1903 - accuracy: 0.9957 - val_loss: 0.5099 - val_accuracy: 0.9295 - lr: 3.9063e-04\n",
            "Epoch 173/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.1902 - accuracy: 0.9955 - val_loss: 0.5202 - val_accuracy: 0.9277 - lr: 3.9063e-04\n",
            "Epoch 174/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.1888 - accuracy: 0.9959 - val_loss: 0.5199 - val_accuracy: 0.9285 - lr: 3.9063e-04\n",
            "Epoch 175/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.1893 - accuracy: 0.9956 - val_loss: 0.5128 - val_accuracy: 0.9290 - lr: 3.9063e-04\n",
            "Epoch 176/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1873 - accuracy: 0.9961 - val_loss: 0.5118 - val_accuracy: 0.9301 - lr: 3.9063e-04\n",
            "Epoch 177/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.1871 - accuracy: 0.9959 - val_loss: 0.5233 - val_accuracy: 0.9268 - lr: 3.9063e-04\n",
            "Epoch 178/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.1862 - accuracy: 0.9963 - val_loss: 0.5183 - val_accuracy: 0.9278 - lr: 3.9063e-04\n",
            "Epoch 179/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.1868 - accuracy: 0.9959 - val_loss: 0.5173 - val_accuracy: 0.9288 - lr: 3.9063e-04\n",
            "Epoch 180/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.1857 - accuracy: 0.9956 - val_loss: 0.5143 - val_accuracy: 0.9289 - lr: 3.9063e-04\n",
            "Epoch 181/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1873 - accuracy: 0.9957 - val_loss: 0.5122 - val_accuracy: 0.9304 - lr: 1.9531e-04\n",
            "Epoch 182/250\n",
            "312/312 [==============================] - 36s 114ms/step - loss: 0.1834 - accuracy: 0.9964 - val_loss: 0.5144 - val_accuracy: 0.9290 - lr: 1.9531e-04\n",
            "Epoch 183/250\n",
            "312/312 [==============================] - 32s 104ms/step - loss: 0.1852 - accuracy: 0.9961 - val_loss: 0.5083 - val_accuracy: 0.9294 - lr: 1.9531e-04\n",
            "Epoch 184/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1827 - accuracy: 0.9966 - val_loss: 0.5102 - val_accuracy: 0.9298 - lr: 1.9531e-04\n",
            "Epoch 185/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1835 - accuracy: 0.9966 - val_loss: 0.5103 - val_accuracy: 0.9299 - lr: 1.9531e-04\n",
            "Epoch 186/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1832 - accuracy: 0.9965 - val_loss: 0.5162 - val_accuracy: 0.9291 - lr: 1.9531e-04\n",
            "Epoch 187/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1834 - accuracy: 0.9964 - val_loss: 0.5065 - val_accuracy: 0.9304 - lr: 1.9531e-04\n",
            "Epoch 188/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.1827 - accuracy: 0.9965 - val_loss: 0.5049 - val_accuracy: 0.9295 - lr: 1.9531e-04\n",
            "Epoch 189/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1826 - accuracy: 0.9965 - val_loss: 0.5070 - val_accuracy: 0.9291 - lr: 1.9531e-04\n",
            "Epoch 190/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1805 - accuracy: 0.9970 - val_loss: 0.5096 - val_accuracy: 0.9289 - lr: 1.9531e-04\n",
            "Epoch 191/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1835 - accuracy: 0.9959 - val_loss: 0.5100 - val_accuracy: 0.9291 - lr: 1.9531e-04\n",
            "Epoch 192/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1812 - accuracy: 0.9969 - val_loss: 0.5127 - val_accuracy: 0.9289 - lr: 1.9531e-04\n",
            "Epoch 193/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.1811 - accuracy: 0.9970 - val_loss: 0.5049 - val_accuracy: 0.9300 - lr: 1.9531e-04\n",
            "Epoch 194/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.1816 - accuracy: 0.9965 - val_loss: 0.5049 - val_accuracy: 0.9307 - lr: 1.9531e-04\n",
            "Epoch 195/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1795 - accuracy: 0.9969 - val_loss: 0.5105 - val_accuracy: 0.9306 - lr: 1.9531e-04\n",
            "Epoch 196/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.1806 - accuracy: 0.9968 - val_loss: 0.5273 - val_accuracy: 0.9278 - lr: 1.9531e-04\n",
            "Epoch 197/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1808 - accuracy: 0.9967 - val_loss: 0.5159 - val_accuracy: 0.9300 - lr: 1.9531e-04\n",
            "Epoch 198/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1803 - accuracy: 0.9966 - val_loss: 0.5083 - val_accuracy: 0.9312 - lr: 1.9531e-04\n",
            "Epoch 199/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.1794 - accuracy: 0.9971 - val_loss: 0.5139 - val_accuracy: 0.9297 - lr: 1.9531e-04\n",
            "Epoch 200/250\n",
            "312/312 [==============================] - 36s 114ms/step - loss: 0.1798 - accuracy: 0.9965 - val_loss: 0.5141 - val_accuracy: 0.9297 - lr: 1.9531e-04\n",
            "Epoch 201/250\n",
            "312/312 [==============================] - 36s 115ms/step - loss: 0.1806 - accuracy: 0.9963 - val_loss: 0.5125 - val_accuracy: 0.9302 - lr: 9.7656e-05\n",
            "Epoch 202/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1785 - accuracy: 0.9971 - val_loss: 0.5113 - val_accuracy: 0.9297 - lr: 9.7656e-05\n",
            "Epoch 203/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1796 - accuracy: 0.9966 - val_loss: 0.5145 - val_accuracy: 0.9303 - lr: 9.7656e-05\n",
            "Epoch 204/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.1797 - accuracy: 0.9965 - val_loss: 0.5110 - val_accuracy: 0.9295 - lr: 9.7656e-05\n",
            "Epoch 205/250\n",
            "312/312 [==============================] - 32s 104ms/step - loss: 0.1795 - accuracy: 0.9968 - val_loss: 0.5106 - val_accuracy: 0.9296 - lr: 9.7656e-05\n",
            "Epoch 206/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1765 - accuracy: 0.9974 - val_loss: 0.5088 - val_accuracy: 0.9301 - lr: 9.7656e-05\n",
            "Epoch 207/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.1783 - accuracy: 0.9971 - val_loss: 0.5076 - val_accuracy: 0.9300 - lr: 9.7656e-05\n",
            "Epoch 208/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1772 - accuracy: 0.9969 - val_loss: 0.5130 - val_accuracy: 0.9305 - lr: 9.7656e-05\n",
            "Epoch 209/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1777 - accuracy: 0.9966 - val_loss: 0.5103 - val_accuracy: 0.9303 - lr: 9.7656e-05\n",
            "Epoch 210/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1773 - accuracy: 0.9972 - val_loss: 0.5111 - val_accuracy: 0.9299 - lr: 9.7656e-05\n",
            "Epoch 211/250\n",
            "312/312 [==============================] - 43s 138ms/step - loss: 0.1779 - accuracy: 0.9971 - val_loss: 0.5094 - val_accuracy: 0.9305 - lr: 9.7656e-05\n",
            "Epoch 212/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.1776 - accuracy: 0.9970 - val_loss: 0.5070 - val_accuracy: 0.9307 - lr: 9.7656e-05\n",
            "Epoch 213/250\n",
            "312/312 [==============================] - 36s 114ms/step - loss: 0.1774 - accuracy: 0.9970 - val_loss: 0.5100 - val_accuracy: 0.9303 - lr: 9.7656e-05\n",
            "Epoch 214/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.1766 - accuracy: 0.9972 - val_loss: 0.5140 - val_accuracy: 0.9302 - lr: 9.7656e-05\n",
            "Epoch 215/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1770 - accuracy: 0.9971 - val_loss: 0.5101 - val_accuracy: 0.9306 - lr: 9.7656e-05\n",
            "Epoch 216/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1768 - accuracy: 0.9971 - val_loss: 0.5048 - val_accuracy: 0.9298 - lr: 9.7656e-05\n",
            "Epoch 217/250\n",
            "312/312 [==============================] - 36s 114ms/step - loss: 0.1767 - accuracy: 0.9971 - val_loss: 0.5058 - val_accuracy: 0.9304 - lr: 9.7656e-05\n",
            "Epoch 218/250\n",
            "312/312 [==============================] - 37s 117ms/step - loss: 0.1767 - accuracy: 0.9971 - val_loss: 0.5058 - val_accuracy: 0.9305 - lr: 9.7656e-05\n",
            "Epoch 219/250\n",
            "312/312 [==============================] - 34s 110ms/step - loss: 0.1753 - accuracy: 0.9973 - val_loss: 0.5080 - val_accuracy: 0.9293 - lr: 9.7656e-05\n",
            "Epoch 220/250\n",
            "312/312 [==============================] - 32s 104ms/step - loss: 0.1771 - accuracy: 0.9972 - val_loss: 0.5092 - val_accuracy: 0.9304 - lr: 9.7656e-05\n",
            "Epoch 221/250\n",
            "312/312 [==============================] - 36s 114ms/step - loss: 0.1760 - accuracy: 0.9971 - val_loss: 0.5096 - val_accuracy: 0.9299 - lr: 4.8828e-05\n",
            "Epoch 222/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.1753 - accuracy: 0.9973 - val_loss: 0.5084 - val_accuracy: 0.9306 - lr: 4.8828e-05\n",
            "Epoch 223/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1770 - accuracy: 0.9969 - val_loss: 0.5091 - val_accuracy: 0.9298 - lr: 4.8828e-05\n",
            "Epoch 224/250\n",
            "312/312 [==============================] - 33s 106ms/step - loss: 0.1759 - accuracy: 0.9972 - val_loss: 0.5106 - val_accuracy: 0.9301 - lr: 4.8828e-05\n",
            "Epoch 225/250\n",
            "312/312 [==============================] - 35s 111ms/step - loss: 0.1747 - accuracy: 0.9973 - val_loss: 0.5102 - val_accuracy: 0.9299 - lr: 4.8828e-05\n",
            "Epoch 226/250\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 0.1761 - accuracy: 0.9971 - val_loss: 0.5116 - val_accuracy: 0.9297 - lr: 4.8828e-05\n",
            "Epoch 227/250\n",
            "312/312 [==============================] - 33s 106ms/step - loss: 0.1764 - accuracy: 0.9970 - val_loss: 0.5094 - val_accuracy: 0.9299 - lr: 4.8828e-05\n",
            "Epoch 228/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1757 - accuracy: 0.9973 - val_loss: 0.5080 - val_accuracy: 0.9305 - lr: 4.8828e-05\n",
            "Epoch 229/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.1758 - accuracy: 0.9970 - val_loss: 0.5080 - val_accuracy: 0.9307 - lr: 4.8828e-05\n",
            "Epoch 230/250\n",
            "312/312 [==============================] - 33s 107ms/step - loss: 0.1761 - accuracy: 0.9970 - val_loss: 0.5081 - val_accuracy: 0.9302 - lr: 4.8828e-05\n",
            "Epoch 231/250\n",
            "312/312 [==============================] - 36s 115ms/step - loss: 0.1747 - accuracy: 0.9974 - val_loss: 0.5093 - val_accuracy: 0.9303 - lr: 4.8828e-05\n",
            "Epoch 232/250\n",
            "312/312 [==============================] - 33s 107ms/step - loss: 0.1757 - accuracy: 0.9971 - val_loss: 0.5078 - val_accuracy: 0.9305 - lr: 4.8828e-05\n",
            "Epoch 233/250\n",
            "312/312 [==============================] - 36s 115ms/step - loss: 0.1748 - accuracy: 0.9972 - val_loss: 0.5113 - val_accuracy: 0.9306 - lr: 4.8828e-05\n",
            "Epoch 234/250\n",
            "312/312 [==============================] - 35s 112ms/step - loss: 0.1756 - accuracy: 0.9972 - val_loss: 0.5092 - val_accuracy: 0.9302 - lr: 4.8828e-05\n",
            "Epoch 235/250\n",
            "312/312 [==============================] - 33s 104ms/step - loss: 0.1757 - accuracy: 0.9970 - val_loss: 0.5064 - val_accuracy: 0.9308 - lr: 4.8828e-05\n",
            "Epoch 236/250\n",
            "312/312 [==============================] - 36s 115ms/step - loss: 0.1744 - accuracy: 0.9975 - val_loss: 0.5073 - val_accuracy: 0.9302 - lr: 4.8828e-05\n",
            "Epoch 237/250\n",
            "312/312 [==============================] - 36s 116ms/step - loss: 0.1751 - accuracy: 0.9972 - val_loss: 0.5073 - val_accuracy: 0.9307 - lr: 4.8828e-05\n",
            "Epoch 238/250\n",
            "312/312 [==============================] - 36s 115ms/step - loss: 0.1746 - accuracy: 0.9971 - val_loss: 0.5065 - val_accuracy: 0.9306 - lr: 4.8828e-05\n",
            "Epoch 239/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1743 - accuracy: 0.9972 - val_loss: 0.5072 - val_accuracy: 0.9305 - lr: 4.8828e-05\n",
            "Epoch 240/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.1739 - accuracy: 0.9974 - val_loss: 0.5083 - val_accuracy: 0.9307 - lr: 4.8828e-05\n",
            "Epoch 241/250\n",
            "312/312 [==============================] - 36s 115ms/step - loss: 0.1738 - accuracy: 0.9976 - val_loss: 0.5061 - val_accuracy: 0.9309 - lr: 2.4414e-05\n",
            "Epoch 242/250\n",
            "312/312 [==============================] - 35s 113ms/step - loss: 0.1741 - accuracy: 0.9976 - val_loss: 0.5080 - val_accuracy: 0.9303 - lr: 2.4414e-05\n",
            "Epoch 243/250\n",
            "312/312 [==============================] - 33s 107ms/step - loss: 0.1745 - accuracy: 0.9975 - val_loss: 0.5078 - val_accuracy: 0.9307 - lr: 2.4414e-05\n",
            "Epoch 244/250\n",
            "312/312 [==============================] - 36s 116ms/step - loss: 0.1745 - accuracy: 0.9975 - val_loss: 0.5085 - val_accuracy: 0.9308 - lr: 2.4414e-05\n",
            "Epoch 245/250\n",
            "312/312 [==============================] - 36s 116ms/step - loss: 0.1733 - accuracy: 0.9975 - val_loss: 0.5090 - val_accuracy: 0.9303 - lr: 2.4414e-05\n",
            "Epoch 246/250\n",
            "312/312 [==============================] - 37s 117ms/step - loss: 0.1744 - accuracy: 0.9976 - val_loss: 0.5067 - val_accuracy: 0.9307 - lr: 2.4414e-05\n",
            "Epoch 247/250\n",
            "312/312 [==============================] - 34s 108ms/step - loss: 0.1741 - accuracy: 0.9973 - val_loss: 0.5082 - val_accuracy: 0.9308 - lr: 2.4414e-05\n",
            "Epoch 248/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.1748 - accuracy: 0.9974 - val_loss: 0.5076 - val_accuracy: 0.9305 - lr: 2.4414e-05\n",
            "Epoch 249/250\n",
            "312/312 [==============================] - 34s 107ms/step - loss: 0.1750 - accuracy: 0.9973 - val_loss: 0.5073 - val_accuracy: 0.9298 - lr: 2.4414e-05\n",
            "Epoch 250/250\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 0.1748 - accuracy: 0.9971 - val_loss: 0.5071 - val_accuracy: 0.9302 - lr: 2.4414e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Convergence Graph and compare the results\n",
        "# plot loss and accuracy\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r-', label='val_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['accuracy'], 'g-', label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], 'r-', label='val_accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "qbGipjBFrIYY",
        "outputId": "78cc5008-f9b0-4e83-d11e-9f061d0dfa40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,acc = vgg.evaluate(x_test,y_test)\n",
        "print('evaluate loss:%f acc %f'%(loss,acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsjQb9tIrweN",
        "outputId": "0b255a05-c048-4236-840b-f4e2f741ff38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 10ms/step - loss: 0.5266 - accuracy: 0.9317\n",
            "evaluate loss:0.526583 acc 0.931700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ODkZ7CrJjehF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}